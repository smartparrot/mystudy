{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh2eTm/C/xvYFlflLMSbMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartparrot/mystudy/blob/master/GPT2_Chinese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRrkCoaEflSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2509af2-d050-414f-e9bc-f4e17e10416d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPT2-Chinese'...\n",
            "remote: Enumerating objects: 280, done.\u001b[K\n",
            "remote: Total 280 (delta 0), reused 0 (delta 0), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (280/280), 13.44 MiB | 24.62 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Morizeyao/GPT2-Chinese.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd GPT2-Chinese/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdeboIYilxlX",
        "outputId": "06be4728-0e65-4094-dca4-c205198f777e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT2-Chinese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ZgNPenjhKE",
        "outputId": "62cebe3c-5f0a-453e-902a-439cfacd6f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.1.1\n",
            "  Downloading transformers-2.1.1-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (4.64.1)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (2.11.0)\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.13.0a20230211-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (0.16.0)\n",
            "Collecting thulac\n",
            "  Downloading thulac-0.2.2-py3-none-any.whl (53.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==2.1.1->-r requirements.txt (line 1)) (2.25.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from transformers==2.1.1->-r requirements.txt (line 1)) (2022.6.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.26.69-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 2)) (4.4.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (1.8.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (2.16.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (1.51.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (0.38.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (3.19.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 7)) (5.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tb-nightly->-r requirements.txt (line 7)) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.1.1->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.1.1->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.1.1->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.1.1->-r requirements.txt (line 1)) (1.24.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.69\n",
            "  Downloading botocore-1.29.69-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.1.1->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.1.1->-r requirements.txt (line 1)) (1.2.0)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.69->boto3->transformers==2.1.1->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->-r requirements.txt (line 7)) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->-r requirements.txt (line 7)) (3.2.2)\n",
            "Building wheels for collected packages: sklearn, sacremoses\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=244b5a220ddc35a4d7c9a14c7e47fdcb58048a6f8d8da8f751b1fb89b48ae60e\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=c5ef3188ff6dfb5873492f73623d28da8d15a7f0b882b27c64a9c67627069549\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sklearn sacremoses\n",
            "Installing collected packages: thulac, sklearn, sentencepiece, urllib3, tensorboard-data-server, sacremoses, jmespath, botocore, s3transfer, boto3, transformers, tb-nightly\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.11.2 requires tensorboard-data-server<0.7.0,>=0.6.0, but you have tensorboard-data-server 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.26.69 botocore-1.29.69 jmespath-1.0.1 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sklearn-0.0.post1 tb-nightly-2.13.0a20230211 tensorboard-data-server-0.7.0 thulac-0.2.2 transformers-2.1.1 urllib3-1.26.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir data"
      ],
      "metadata": {
        "id": "0GECEgWIiLgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "执行训练"
      ],
      "metadata": {
        "id": "thzvmvYGjSpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --raw --num_pieces 100 --batch_size 4 --epochs 20 --log_step 5 --pretrained_model 'model/final_model/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I5NsTB8jczm",
        "outputId": "46db8ea7-b62c-47ae-cde7-ffd399a7167b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 14:23:49.880216: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 14:23:50.850778: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:23:50.850877: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:23:50.850897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-11 14:23:54.819639: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "args:\n",
            "Namespace(batch_size=4, bpe_token=False, device='0,1,2,3', encoder_json='tokenizations/encoder.json', epochs=20, fp16=False, fp16_opt_level='O1', gradient_accumulation=1, log_step=5, lr=0.00015, max_grad_norm=1.0, min_length=128, model_config='config/model_config_small.json', num_pieces=100, output_dir='model/', pretrained_model='model/final_model/', raw=True, raw_data_path='data/train.json', segment=False, stride=768, tokenized_data_path='data/tokenized/', tokenizer_path='cache/vocab_small.txt', vocab_bpe='tokenizations/vocab.bpe', warmup_steps=2000, writer_dir='tensorboard_summary/')\n",
            "config:\n",
            "{\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 10,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 13317\n",
            "}\n",
            "\n",
            "using device: cuda\n",
            "building files\n",
            "reading lines\n",
            "100% 100/100 [00:01<00:00, 97.87it/s]\n",
            "finish\n",
            "files built\n",
            "number of parameters: 81894144\n",
            "calculating total steps\n",
            "100% 100/100 [00:00<00:00, 3552.06it/s]\n",
            "total steps = 1241\n",
            "starting training\n",
            "epoch 1\n",
            "time: 2023-02-11 14:23:59.559427\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "now time: 14:24. Step 1 of piece 21 of epoch 1, loss 1.302959132194519\n",
            "now time: 14:24. Step 1 of piece 37 of epoch 1, loss 1.7556780338287354\n",
            "now time: 14:24. Step 1 of piece 55 of epoch 1, loss 1.7649311542510986\n",
            "now time: 14:24. Step 3 of piece 59 of epoch 1, loss 1.3037213802337646\n",
            "now time: 14:24. Step 1 of piece 71 of epoch 1, loss 1.9969529390335083\n",
            "now time: 14:24. Step 1 of piece 88 of epoch 1, loss 1.6842303037643434\n",
            "saving model for epoch 1\n",
            "epoch 1 finished\n",
            "time: 2023-02-11 14:24:32.930453\n",
            "time for one epoch: 0:00:33.371026\n",
            "epoch 2\n",
            "time: 2023-02-11 14:24:32.930497\n",
            "now time: 14:24. Step 1 of piece 0 of epoch 2, loss 2.6205564975738525\n",
            "now time: 14:24. Step 1 of piece 8 of epoch 2, loss 1.333289623260498\n",
            "now time: 14:24. Step 1 of piece 31 of epoch 2, loss 1.6132097959518432\n",
            "now time: 14:24. Step 1 of piece 43 of epoch 2, loss 2.1835436820983887\n",
            "now time: 14:24. Step 1 of piece 56 of epoch 2, loss 1.511532688140869\n",
            "now time: 14:24. Step 1 of piece 71 of epoch 2, loss 1.8930723667144775\n",
            "now time: 14:25. Step 1 of piece 81 of epoch 2, loss 1.6277910709381103\n",
            "saving model for epoch 2\n",
            "epoch 2 finished\n",
            "time: 2023-02-11 14:25:06.539960\n",
            "time for one epoch: 0:00:33.609463\n",
            "epoch 3\n",
            "time: 2023-02-11 14:25:06.540012\n",
            "now time: 14:25. Step 1 of piece 3 of epoch 3, loss 1.6339223861694336\n",
            "now time: 14:25. Step 1 of piece 6 of epoch 3, loss 1.302094864845276\n",
            "now time: 14:25. Step 1 of piece 25 of epoch 3, loss 2.1124583005905153\n",
            "now time: 14:25. Step 2 of piece 40 of epoch 3, loss 1.599955177307129\n",
            "now time: 14:25. Step 1 of piece 56 of epoch 3, loss 2.005208134651184\n",
            "now time: 14:25. Step 1 of piece 87 of epoch 3, loss 1.4293568134307861\n",
            "now time: 14:25. Step 1 of piece 96 of epoch 3, loss 1.5406526565551757\n",
            "saving model for epoch 3\n",
            "epoch 3 finished\n",
            "time: 2023-02-11 14:25:40.581153\n",
            "time for one epoch: 0:00:34.041141\n",
            "epoch 4\n",
            "time: 2023-02-11 14:25:40.581201\n",
            "now time: 14:25. Step 1 of piece 6 of epoch 4, loss 1.8747714281082153\n",
            "now time: 14:25. Step 1 of piece 26 of epoch 4, loss 1.4105944633483887\n",
            "now time: 14:25. Step 1 of piece 46 of epoch 4, loss 1.9621208906173706\n",
            "now time: 14:25. Step 1 of piece 49 of epoch 4, loss 1.7691563367843628\n",
            "now time: 14:26. Step 1 of piece 70 of epoch 4, loss 2.076681208610535\n",
            "now time: 14:26. Step 1 of piece 76 of epoch 4, loss 1.2003879070281982\n",
            "now time: 14:26. Step 1 of piece 97 of epoch 4, loss 1.8175029516220094\n",
            "saving model for epoch 4\n",
            "epoch 4 finished\n",
            "time: 2023-02-11 14:26:15.230356\n",
            "time for one epoch: 0:00:34.649155\n",
            "epoch 5\n",
            "time: 2023-02-11 14:26:15.230403\n",
            "now time: 14:26. Step 1 of piece 10 of epoch 5, loss 1.8923904180526734\n",
            "now time: 14:26. Step 1 of piece 26 of epoch 5, loss 1.647508478164673\n",
            "now time: 14:26. Step 1 of piece 40 of epoch 5, loss 1.567918586730957\n",
            "now time: 14:26. Step 1 of piece 48 of epoch 5, loss 2.1726100921630858\n",
            "now time: 14:26. Step 3 of piece 67 of epoch 5, loss 1.4373136281967163\n",
            "now time: 14:26. Step 1 of piece 80 of epoch 5, loss 1.6533275842666626\n",
            "now time: 14:26. Step 3 of piece 97 of epoch 5, loss 1.2143179893493652\n",
            "saving model for epoch 5\n",
            "epoch 5 finished\n",
            "time: 2023-02-11 14:26:50.336487\n",
            "time for one epoch: 0:00:35.106084\n",
            "epoch 6\n",
            "time: 2023-02-11 14:26:50.336531\n",
            "now time: 14:26. Step 1 of piece 12 of epoch 6, loss 1.5326275825500488\n",
            "now time: 14:27. Step 1 of piece 23 of epoch 6, loss 1.9354316473007203\n",
            "now time: 14:27. Step 1 of piece 36 of epoch 6, loss 2.2537091732025147\n",
            "now time: 14:27. Step 1 of piece 48 of epoch 6, loss 1.5413923740386963\n",
            "now time: 14:27. Step 1 of piece 60 of epoch 6, loss 1.3812691688537597\n",
            "now time: 14:27. Step 2 of piece 72 of epoch 6, loss 1.3665289163589478\n",
            "saving model for epoch 6\n",
            "epoch 6 finished\n",
            "time: 2023-02-11 14:27:25.998754\n",
            "time for one epoch: 0:00:35.662223\n",
            "epoch 7\n",
            "time: 2023-02-11 14:27:25.998805\n",
            "now time: 14:27. Step 1 of piece 15 of epoch 7, loss 1.5136274576187134\n",
            "now time: 14:27. Step 3 of piece 23 of epoch 7, loss 1.7476627111434937\n",
            "now time: 14:27. Step 2 of piece 30 of epoch 7, loss 1.5184562444686889\n",
            "now time: 14:27. Step 1 of piece 38 of epoch 7, loss 1.5436240673065185\n",
            "now time: 14:27. Step 1 of piece 49 of epoch 7, loss 1.7390497207641602\n",
            "now time: 14:27. Step 1 of piece 66 of epoch 7, loss 1.731199049949646\n",
            "now time: 14:27. Step 1 of piece 86 of epoch 7, loss 1.7472747325897218\n",
            "saving model for epoch 7\n",
            "epoch 7 finished\n",
            "time: 2023-02-11 14:28:02.175606\n",
            "time for one epoch: 0:00:36.176801\n",
            "epoch 8\n",
            "time: 2023-02-11 14:28:02.175655\n",
            "now time: 14:28. Step 1 of piece 4 of epoch 8, loss 1.5893972635269165\n",
            "now time: 14:28. Step 1 of piece 20 of epoch 8, loss 1.7735700130462646\n",
            "now time: 14:28. Step 1 of piece 36 of epoch 8, loss 1.6373487710952759\n",
            "now time: 14:28. Step 1 of piece 54 of epoch 8, loss 1.5293800830841064\n",
            "now time: 14:28. Step 1 of piece 65 of epoch 8, loss 1.3731208801269532\n",
            "now time: 14:28. Step 1 of piece 72 of epoch 8, loss 1.2218681693077087\n",
            "now time: 14:28. Step 1 of piece 97 of epoch 8, loss 2.0392064571380617\n",
            "saving model for epoch 8\n",
            "epoch 8 finished\n",
            "time: 2023-02-11 14:28:38.528415\n",
            "time for one epoch: 0:00:36.352760\n",
            "epoch 9\n",
            "time: 2023-02-11 14:28:38.528465\n",
            "now time: 14:28. Step 1 of piece 18 of epoch 9, loss 1.8733702659606934\n",
            "now time: 14:28. Step 1 of piece 31 of epoch 9, loss 1.4798676252365113\n",
            "now time: 14:28. Step 1 of piece 36 of epoch 9, loss 1.2725132703781128\n",
            "now time: 14:28. Step 1 of piece 47 of epoch 9, loss 1.6168336153030396\n",
            "now time: 14:29. Step 1 of piece 62 of epoch 9, loss 1.1496635317802428\n",
            "now time: 14:29. Step 1 of piece 76 of epoch 9, loss 1.8889441013336181\n",
            "now time: 14:29. Step 1 of piece 98 of epoch 9, loss 1.6894088506698608\n",
            "saving model for epoch 9\n",
            "epoch 9 finished\n",
            "time: 2023-02-11 14:29:14.608199\n",
            "time for one epoch: 0:00:36.079734\n",
            "epoch 10\n",
            "time: 2023-02-11 14:29:14.608254\n",
            "now time: 14:29. Step 1 of piece 11 of epoch 10, loss 2.0111717224121093\n",
            "now time: 14:29. Step 1 of piece 17 of epoch 10, loss 1.325047242641449\n",
            "now time: 14:29. Step 1 of piece 24 of epoch 10, loss 1.2502864360809327\n",
            "now time: 14:29. Step 1 of piece 33 of epoch 10, loss 1.5665313482284546\n",
            "now time: 14:29. Step 1 of piece 45 of epoch 10, loss 1.9321692705154419\n",
            "now time: 14:29. Step 1 of piece 66 of epoch 10, loss 1.5659886360168458\n",
            "now time: 14:29. Step 1 of piece 93 of epoch 10, loss 1.3930949687957763\n",
            "saving model for epoch 10\n",
            "epoch 10 finished\n",
            "time: 2023-02-11 14:29:50.636058\n",
            "time for one epoch: 0:00:36.027804\n",
            "epoch 11\n",
            "time: 2023-02-11 14:29:50.636108\n",
            "now time: 14:29. Step 1 of piece 14 of epoch 11, loss 1.2734988927841187\n",
            "now time: 14:30. Step 1 of piece 31 of epoch 11, loss 1.941435742378235\n",
            "now time: 14:30. Step 1 of piece 54 of epoch 11, loss 1.673686933517456\n",
            "now time: 14:30. Step 1 of piece 73 of epoch 11, loss 1.7617206811904906\n",
            "now time: 14:30. Step 1 of piece 80 of epoch 11, loss 1.6561971664428712\n",
            "now time: 14:30. Step 1 of piece 91 of epoch 11, loss 1.3601208209991456\n",
            "saving model for epoch 11\n",
            "epoch 11 finished\n",
            "time: 2023-02-11 14:30:26.998188\n",
            "time for one epoch: 0:00:36.362080\n",
            "epoch 12\n",
            "time: 2023-02-11 14:30:26.998255\n",
            "now time: 14:30. Step 1 of piece 0 of epoch 12, loss 1.1227488040924072\n",
            "now time: 14:30. Step 3 of piece 23 of epoch 12, loss 1.330192995071411\n",
            "now time: 14:30. Step 2 of piece 36 of epoch 12, loss 1.5898276090621948\n",
            "now time: 14:30. Step 2 of piece 57 of epoch 12, loss 1.2814924359321593\n",
            "now time: 14:30. Step 1 of piece 68 of epoch 12, loss 1.439487600326538\n",
            "now time: 14:30. Step 1 of piece 78 of epoch 12, loss 1.7227534770965576\n",
            "now time: 14:30. Step 1 of piece 87 of epoch 12, loss 1.6224957704544067\n",
            "saving model for epoch 12\n",
            "epoch 12 finished\n",
            "time: 2023-02-11 14:31:03.304856\n",
            "time for one epoch: 0:00:36.306601\n",
            "epoch 13\n",
            "time: 2023-02-11 14:31:03.304905\n",
            "now time: 14:31. Step 1 of piece 7 of epoch 13, loss 1.4702916383743285\n",
            "now time: 14:31. Step 1 of piece 20 of epoch 13, loss 1.8279594421386718\n",
            "now time: 14:31. Step 1 of piece 37 of epoch 13, loss 1.551905918121338\n",
            "now time: 14:31. Step 1 of piece 62 of epoch 13, loss 1.8871700406074523\n",
            "now time: 14:31. Step 2 of piece 67 of epoch 13, loss 1.2755771279335022\n",
            "now time: 14:31. Step 1 of piece 77 of epoch 13, loss 1.27465922832489\n",
            "now time: 14:31. Step 1 of piece 93 of epoch 13, loss 1.3365647315979003\n",
            "saving model for epoch 13\n",
            "epoch 13 finished\n",
            "time: 2023-02-11 14:31:39.696030\n",
            "time for one epoch: 0:00:36.391125\n",
            "epoch 14\n",
            "time: 2023-02-11 14:31:39.696082\n",
            "now time: 14:31. Step 2 of piece 10 of epoch 14, loss 1.1196219325065613\n",
            "now time: 14:31. Step 1 of piece 34 of epoch 14, loss 1.6725162506103515\n",
            "now time: 14:31. Step 1 of piece 45 of epoch 14, loss 1.427835965156555\n",
            "now time: 14:31. Step 1 of piece 51 of epoch 14, loss 1.5082882165908813\n",
            "now time: 14:32. Step 1 of piece 75 of epoch 14, loss 1.5885390520095826\n",
            "now time: 14:32. Step 1 of piece 87 of epoch 14, loss 1.3604801416397094\n",
            "now time: 14:32. Step 1 of piece 96 of epoch 14, loss 1.4193611025810242\n",
            "saving model for epoch 14\n",
            "epoch 14 finished\n",
            "time: 2023-02-11 14:32:16.110531\n",
            "time for one epoch: 0:00:36.414449\n",
            "epoch 15\n",
            "time: 2023-02-11 14:32:16.110567\n",
            "now time: 14:32. Step 1 of piece 4 of epoch 15, loss 1.5718994855880737\n",
            "now time: 14:32. Step 1 of piece 25 of epoch 15, loss 1.0845010638237\n",
            "now time: 14:32. Step 1 of piece 34 of epoch 15, loss 1.1892964005470277\n",
            "now time: 14:32. Step 1 of piece 47 of epoch 15, loss 1.732901120185852\n",
            "now time: 14:32. Step 1 of piece 57 of epoch 15, loss 1.2544555425643922\n",
            "now time: 14:32. Step 1 of piece 70 of epoch 15, loss 1.873100209236145\n",
            "now time: 14:32. Step 1 of piece 97 of epoch 15, loss 1.3486724972724915\n",
            "saving model for epoch 15\n",
            "epoch 15 finished\n",
            "time: 2023-02-11 14:32:52.441756\n",
            "time for one epoch: 0:00:36.331189\n",
            "epoch 16\n",
            "time: 2023-02-11 14:32:52.441807\n",
            "now time: 14:32. Step 1 of piece 15 of epoch 16, loss 1.8247987151145935\n",
            "now time: 14:33. Step 1 of piece 29 of epoch 16, loss 1.4020742416381835\n",
            "now time: 14:33. Step 2 of piece 53 of epoch 16, loss 1.0611177444458009\n",
            "now time: 14:33. Step 1 of piece 55 of epoch 16, loss 0.9875593423843384\n",
            "now time: 14:33. Step 1 of piece 75 of epoch 16, loss 1.439029908180237\n",
            "now time: 14:33. Step 1 of piece 86 of epoch 16, loss 1.2873062133789062\n",
            "saving model for epoch 16\n",
            "epoch 16 finished\n",
            "time: 2023-02-11 14:33:28.516526\n",
            "time for one epoch: 0:00:36.074719\n",
            "epoch 17\n",
            "time: 2023-02-11 14:33:28.516569\n",
            "now time: 14:33. Step 1 of piece 0 of epoch 17, loss 1.7369922876358033\n",
            "now time: 14:33. Step 1 of piece 17 of epoch 17, loss 1.066601324081421\n",
            "now time: 14:33. Step 2 of piece 27 of epoch 17, loss 1.2937482953071595\n",
            "now time: 14:33. Step 1 of piece 38 of epoch 17, loss 1.478560245037079\n",
            "now time: 14:33. Step 1 of piece 53 of epoch 17, loss 1.3071826696395874\n",
            "now time: 14:33. Step 1 of piece 61 of epoch 17, loss 1.6627794623374939\n",
            "now time: 14:34. Step 3 of piece 70 of epoch 17, loss 1.357697308063507\n",
            "saving model for epoch 17\n",
            "epoch 17 finished\n",
            "time: 2023-02-11 14:34:04.680146\n",
            "time for one epoch: 0:00:36.163577\n",
            "epoch 18\n",
            "time: 2023-02-11 14:34:04.680192\n",
            "now time: 14:34. Step 1 of piece 6 of epoch 18, loss 1.4360084295272828\n",
            "now time: 14:34. Step 2 of piece 15 of epoch 18, loss 1.4040749430656434\n",
            "now time: 14:34. Step 2 of piece 30 of epoch 18, loss 1.3961547136306762\n",
            "now time: 14:34. Step 1 of piece 40 of epoch 18, loss 1.1183157563209534\n",
            "now time: 14:34. Step 1 of piece 51 of epoch 18, loss 1.2007646918296815\n",
            "now time: 14:34. Step 1 of piece 77 of epoch 18, loss 1.519890594482422\n",
            "now time: 14:34. Step 1 of piece 92 of epoch 18, loss 1.1733386874198914\n",
            "saving model for epoch 18\n",
            "epoch 18 finished\n",
            "time: 2023-02-11 14:34:40.918641\n",
            "time for one epoch: 0:00:36.238449\n",
            "epoch 19\n",
            "time: 2023-02-11 14:34:40.918689\n",
            "now time: 14:34. Step 1 of piece 11 of epoch 19, loss 1.3757216930389404\n",
            "now time: 14:34. Step 1 of piece 34 of epoch 19, loss 1.8545735359191895\n",
            "now time: 14:34. Step 1 of piece 43 of epoch 19, loss 1.1853263258934021\n",
            "now time: 14:34. Step 1 of piece 54 of epoch 19, loss 1.034144651889801\n",
            "now time: 14:35. Step 1 of piece 71 of epoch 19, loss 1.178651762008667\n",
            "now time: 14:35. Step 1 of piece 84 of epoch 19, loss 1.2226157069206238\n",
            "now time: 14:35. Step 1 of piece 93 of epoch 19, loss 1.2104671716690063\n",
            "saving model for epoch 19\n",
            "epoch 19 finished\n",
            "time: 2023-02-11 14:35:17.294122\n",
            "time for one epoch: 0:00:36.375433\n",
            "epoch 20\n",
            "time: 2023-02-11 14:35:17.294173\n",
            "now time: 14:35. Step 1 of piece 8 of epoch 20, loss 1.2907572269439698\n",
            "now time: 14:35. Step 1 of piece 19 of epoch 20, loss 1.4901223659515381\n",
            "now time: 14:35. Step 2 of piece 35 of epoch 20, loss 1.4444018363952638\n",
            "now time: 14:35. Step 3 of piece 42 of epoch 20, loss 0.8318704724311828\n",
            "now time: 14:35. Step 1 of piece 62 of epoch 20, loss 1.3122024655342102\n",
            "now time: 14:35. Step 1 of piece 74 of epoch 20, loss 1.2469854354858398\n",
            "now time: 14:35. Step 1 of piece 96 of epoch 20, loss 1.2960358619689942\n",
            "saving model for epoch 20\n",
            "epoch 20 finished\n",
            "time: 2023-02-11 14:35:53.816627\n",
            "time for one epoch: 0:00:36.522454\n",
            "training finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate.py --length=50 --nsamples=4 --prefix=\"我爱你\" --fast_pattern --save_samples --save_samples_path=mnt/xx\n",
        "# epochs = 60\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoIsoggbpZBH",
        "outputId": "9b271b47-f757-49ec-c0e8-293f5833c8fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 14:04:38.700649: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 14:04:40.376082: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:04:40.376826: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:04:40.376853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-11 14:04:48.152922: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "args:\n",
            "Namespace(batch_size=1, device='0,1,2,3', fast_pattern=True, length=50, model_config='config/model_config_small.json', model_path='model/final_model', no_wordpiece=False, nsamples=4, prefix='我爱你', repetition_penalty=1.0, save_samples=True, save_samples_path='mnt/xx', segment=False, temperature=1, tokenizer_path='cache/vocab_small.txt', topk=8, topp=0)\n",
            "100% 50/50 [00:00<00:00, 92.07it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "我爱你，万山万与。相与东风青云。向天下，是了闲花去。\n",
            "\n",
            "有五先生天云，青山光。又何苦。是，可可记。来前度。\n",
            "100% 50/50 [00:00<00:00, 104.32it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "我爱你，一归如家，青梅[UNK]。青山、翠。飞了。自是花小，却共乐，恨天万里，不知此。长年。\n",
            "\n",
            "问讯花飘然、玉立翠\n",
            "100% 50/50 [00:00<00:00, 125.79it/s]\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "我爱你风尘归去，对相如老今青云。叹当时。\n",
            "\n",
            "看花舞，竹子，自古，闲教来，不妨新句。一笑，依然。记。叹小诗，\n",
            "100% 50/50 [00:00<00:00, 124.98it/s]\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "我爱你一归山家，青青云天是。\n",
            "\n",
            "问海天外，对春又何人人间、对苍星。一[UNK]。看尘土。问谁、岁寒梅花朝来世。惊，\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate.py --length=50 --nsamples=4 --prefix=\"我爱你\" --fast_pattern --save_samples --save_samples_path=mnt/xx\n",
        "# epochs = 80\n"
      ],
      "metadata": {
        "id": "fH3Du8PIplwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaaa376b-9224-4dc2-d0e6-94db996194ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 14:22:18.061810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 14:22:19.287732: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:22:19.287864: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:22:19.287883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-11 14:22:25.679645: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "args:\n",
            "Namespace(batch_size=1, device='0,1,2,3', fast_pattern=True, length=50, model_config='config/model_config_small.json', model_path='model/final_model', no_wordpiece=False, nsamples=4, prefix='我爱你', repetition_penalty=1.0, save_samples=True, save_samples_path='mnt/xx', segment=False, temperature=1, tokenizer_path='cache/vocab_small.txt', topk=8, topp=0)\n",
            "100% 50/50 [00:00<00:00, 115.16it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "我爱你，海归去後。相思江，雪飞三又还又底。\n",
            "\n",
            "对流落，依然谁是记红青山到东风仙暮。正应合。正想东风、花飞，\n",
            "100% 50/50 [00:00<00:00, 121.38it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "我爱你，[UNK]归去无思、初见。青舞。又何时、正好相期。人物似刘。算一似好，过江山闲得我不如今。奈语。\n",
            "\n",
            "烟霞，\n",
            "100% 50/50 [00:00<00:00, 119.30it/s]\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "我爱你山归归去，怕二非者，对红花家。青鸟三窗，还知道无似梦。\n",
            "\n",
            "断江，对白旧家林似，珠帘[UNK]，好与。忆前度霜\n",
            "100% 50/50 [00:00<00:00, 125.53it/s]\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "我爱你，空喜[UNK]归去，青丝後何时。\n",
            "\n",
            "又是先路。问先生，应忆後，雪生春华无奈红光，空里。正雪浪如东风、犹记相\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate.py --length=50 --nsamples=4 --prefix=\"我爱你\" --fast_pattern --save_samples --save_samples_path=mnt/xx\n",
        "# epochs = 100"
      ],
      "metadata": {
        "id": "0ZkHCEO_5qed",
        "outputId": "2e8e5be6-02bb-4701-bad8-014c076338b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 14:36:07.728344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 14:36:08.991248: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:36:08.991395: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 14:36:08.991413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-11 14:36:14.561170: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "args:\n",
            "Namespace(batch_size=1, device='0,1,2,3', fast_pattern=True, length=50, model_config='config/model_config_small.json', model_path='model/final_model', no_wordpiece=False, nsamples=4, prefix='我爱你', repetition_penalty=1.0, save_samples=True, save_samples_path='mnt/xx', segment=False, temperature=1, tokenizer_path='cache/vocab_small.txt', topk=8, topp=0)\n",
            "100% 50/50 [00:00<00:00, 120.97it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "我爱你，一对江水，应寄春。长年，一洲花传世何许。\n",
            "\n",
            "看对景还句。想梅、晓来大化作、亦如何用，文园林怀。好语\n",
            "100% 50/50 [00:00<00:00, 124.50it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "我爱你，深报儿归地，寄红戏何许。奈三杯语。\n",
            "\n",
            "对流流流流未老，想当时、应间手种。是谁肯教今年好。但得花句。\n",
            "100% 50/50 [00:00<00:00, 120.34it/s]\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "我爱你，海初瘦否。相逢家地，是花飞花丝红万顷。这一笑，对流後，也喜尽高间儿女。但白发如此闲喜。丹青帘影。便\n",
            "100% 50/50 [00:00<00:00, 123.54it/s]\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "我爱你，一药登家句。忆来寄，是先言处。\n",
            "\n",
            "薄君王东风道、羞五日暮。对度。对山波寒、绿树色。是千古惟有微雨。\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}